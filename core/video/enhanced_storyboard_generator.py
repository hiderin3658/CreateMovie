#!/usr/bin/env python3
"""
Enhanced Storyboard Generator
PDFã‚¬ã‚¤ãƒ‰ã¨ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹çµ±åˆã«ã‚ˆã‚‹é«˜åº¦åŒ–ã‚·ã‚¹ãƒ†ãƒ 
"""
import json
import re
import random
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum

from .storyboard_generator import CoreStoryboardGenerator, CutData, StoryboardData


class EmotionalPhase(Enum):
    """J-popæ§‹æˆã«ã‚ˆã‚‹æ„Ÿæƒ…ãƒ•ã‚§ãƒ¼ã‚º"""
    SABI_HOOK = "sabi_hook"      # ã‚µãƒ“ï¼ˆãƒ•ãƒƒã‚¯ï¼‰
    A_MELO = "a_melo"            # Aãƒ¡ãƒ­
    B_MELO = "b_melo"            # Bãƒ¡ãƒ­
    SABI_CLIMAX = "sabi_climax"  # ã‚µãƒ“ï¼ˆã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹ï¼‰


class AspectRatio(Enum):
    """ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”"""
    HORIZONTAL = "16:9"
    VERTICAL = "9:16"
    SQUARE = "1:1"
    CINEMATIC = "2.35:1"


class VideoGenre(Enum):
    """å‹•ç”»ã‚¸ãƒ£ãƒ³ãƒ«"""
    EDUCATIONAL = "educational"
    COMMERCIAL = "commercial"
    NARRATIVE = "narrative"
    DOCUMENTARY = "documentary"
    TOURISM = "tourism"
    MUSIC = "music"


@dataclass
class SceneContext:
    """ã‚·ãƒ¼ãƒ³æ–‡è„ˆæƒ…å ±"""
    emotional_phase: EmotionalPhase
    intensity: float  # 0.0-1.0
    pacing: str  # slow, medium, fast
    mood: str
    genre: VideoGenre
    aspect_ratio: AspectRatio
    target_audience: str = "general"


class IntelligentSelectionEngine:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé¸æŠã‚¨ãƒ³ã‚¸ãƒ³"""
    
    # ãƒ ãƒ¼ãƒ‰åˆ¥ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«ï¼ˆãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹æ´»ç”¨ï¼‰
    MOOD_CAMERA_MATRIX = {
        'peaceful': {'primary': ['MS', 'LS'], 'secondary': ['ELS'], 'avoid': ['ECU']},
        'energetic': {'primary': ['MS', 'CU'], 'secondary': ['LS'], 'avoid': ['ELS']},
        'tense': {'primary': ['CU', 'ECU'], 'secondary': ['MS'], 'avoid': ['ELS', 'LS']},
        'intimate': {'primary': ['CU', 'MCU'], 'secondary': ['MS'], 'avoid': ['ELS']},
        'epic': {'primary': ['ELS', 'LS'], 'secondary': ['MS'], 'avoid': ['ECU']},
        'mysterious': {'primary': ['MS', 'CU'], 'secondary': ['LS'], 'avoid': ['ELS']},
        'dramatic': {'primary': ['CU', 'ECU'], 'secondary': ['MCU'], 'avoid': ['ELS']},
        'joyful': {'primary': ['MS', 'LS'], 'secondary': ['MCU'], 'avoid': ['ECU']}
    }
    
    # ãƒ ãƒ¼ãƒ‰åˆ¥æ§‹å›³ï¼ˆãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‹ã‚‰ï¼‰
    MOOD_COMPOSITION_MATRIX = {
        'peaceful': {'primary': ['rule_of_thirds', 'symmetry'], 'secondary': ['negative_space'], 'avoid': ['diagonal']},
        'energetic': {'primary': ['diagonal', 'dynamic_angles'], 'secondary': ['rule_of_thirds'], 'avoid': ['centered']},
        'formal': {'primary': ['centered', 'symmetry'], 'secondary': ['rule_of_thirds'], 'avoid': ['diagonal']},
        'intimate': {'primary': ['centered_tight', 'close_framing'], 'secondary': ['golden_ratio'], 'avoid': ['negative_space']},
        'lonely': {'primary': ['negative_space', 'small_centered'], 'secondary': ['rule_of_thirds'], 'avoid': ['symmetry']},
        'dramatic': {'primary': ['diagonal', 'low_angle'], 'secondary': ['frame_within_frame'], 'avoid': ['symmetry']},
        'serene': {'primary': ['golden_ratio', 'symmetry'], 'secondary': ['horizontal_lines'], 'avoid': ['diagonal']}
    }
    
    # ãƒ ãƒ¼ãƒ‰åˆ¥ã‚«ãƒ¡ãƒ©ãƒ ãƒ¼ãƒ–ãƒ¡ãƒ³ãƒˆï¼ˆãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‹ã‚‰ï¼‰
    MOOD_MOVEMENT_MATRIX = {
        'calm': {'primary': ['static', 'slow_dolly'], 'secondary': ['gentle_pan'], 'avoid': ['handheld', 'fast']},
        'tense': {'primary': ['handheld', 'slow_zoom_in'], 'secondary': ['static'], 'avoid': ['smooth_crane']},
        'exciting': {'primary': ['tracking', 'handheld'], 'secondary': ['fast_pan'], 'avoid': ['static']},
        'intimate': {'primary': ['slow_dolly_in', 'static'], 'secondary': ['subtle_zoom'], 'avoid': ['crane', 'fast']},
        'epic': {'primary': ['crane', 'sweeping'], 'secondary': ['slow_dolly'], 'avoid': ['static', 'handheld']},
        'mysterious': {'primary': ['slow_dolly', 'creeping'], 'secondary': ['gentle_pan'], 'avoid': ['fast', 'jerky']},
        'chaotic': {'primary': ['intense_handheld', 'fast'], 'secondary': ['quick_pan'], 'avoid': ['static', 'smooth']}
    }
    
    # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥å„ªå…ˆæ§‹å›³
    GENRE_COMPOSITION_PREFERENCES = {
        VideoGenre.EDUCATIONAL: ['rule_of_thirds', 'centered'],
        VideoGenre.COMMERCIAL: ['golden_ratio', 'centered', 'negative_space'],
        VideoGenre.NARRATIVE: 'all_types',  # ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã«å¿œã˜ã¦å¤‰å‹•
        VideoGenre.DOCUMENTARY: ['rule_of_thirds'],  # è‡ªç„¶
        VideoGenre.TOURISM: ['rule_of_thirds', 'golden_ratio', 'leading_lines'],
        VideoGenre.MUSIC: ['diagonal', 'dynamic_angles', 'symmetry']
    }
    
    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”åˆ¥æœ€é©åŒ–
    ASPECT_RATIO_OPTIMIZATIONS = {
        AspectRatio.VERTICAL: {
            'preferred_compositions': ['centered', 'rule_of_thirds_vertical', 'negative_space_vertical'],
            'avoid_compositions': ['wide_diagonal'],
            'camera_considerations': 'vertical_flow_priority'
        },
        AspectRatio.HORIZONTAL: {
            'preferred_compositions': ['rule_of_thirds', 'golden_ratio', 'leading_lines'],
            'avoid_compositions': [],
            'camera_considerations': 'horizontal_flow_priority'
        }
    }

    def select_camera_angle(self, context: SceneContext, scene_type: str) -> str:
        """æ–‡è„ˆã«åŸºã¥ã„ãŸã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«é¸æŠ"""
        mood = context.mood.lower()
        intensity = context.intensity
        
        # ãƒ ãƒ¼ãƒ‰å„ªå…ˆé¸æŠ
        if mood in self.MOOD_CAMERA_MATRIX:
            mood_prefs = self.MOOD_CAMERA_MATRIX[mood]
            candidates = mood_prefs['primary'].copy()
            
            # å¼·åº¦ã«åŸºã¥ãèª¿æ•´
            if intensity > 0.7:  # é«˜å¼·åº¦
                if 'CU' in candidates or 'ECU' in candidates:
                    candidates = [c for c in candidates if c in ['CU', 'ECU']]
                else:
                    candidates.append('CU')
            elif intensity < 0.3:  # ä½å¼·åº¦
                candidates = [c for c in candidates if c in ['ELS', 'LS', 'MS']]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®åŸºæœ¬ãƒ«ãƒ¼ãƒ«
            basic_rules = {
                'establishing': 'ELS', 'character_intro': 'MS', 'dialogue': 'MS',
                'action': 'LS', 'emotion': 'CU', 'conclusion': 'LS'
            }
            return basic_rules.get(scene_type, 'MS')
        
        # ãƒ©ãƒ³ãƒ€ãƒ é¸æŠï¼ˆé‡ã¿ä»˜ãã§ãã‚‹ï¼‰
        return random.choice(candidates) if candidates else 'MS'

    def select_composition(self, context: SceneContext, scene_type: str) -> str:
        """æ–‡è„ˆã«åŸºã¥ã„ãŸã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªæ§‹å›³é¸æŠ"""
        mood = context.mood.lower()
        genre = context.genre
        aspect_ratio = context.aspect_ratio
        
        candidates = []
        
        # ãƒ ãƒ¼ãƒ‰å„ªå…ˆ
        if mood in self.MOOD_COMPOSITION_MATRIX:
            mood_prefs = self.MOOD_COMPOSITION_MATRIX[mood]
            candidates.extend(mood_prefs['primary'])
        
        # ã‚¸ãƒ£ãƒ³ãƒ«èª¿æ•´
        if genre in self.GENRE_COMPOSITION_PREFERENCES:
            genre_prefs = self.GENRE_COMPOSITION_PREFERENCES[genre]
            if genre_prefs != 'all_types':
                candidates = [c for c in candidates if c in genre_prefs]
        
        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”æœ€é©åŒ–
        if aspect_ratio in self.ASPECT_RATIO_OPTIMIZATIONS:
            ratio_prefs = self.ASPECT_RATIO_OPTIMIZATIONS[aspect_ratio]
            preferred = ratio_prefs['preferred_compositions']
            candidates = [c for c in candidates if c in preferred] or candidates
        
        return random.choice(candidates) if candidates else 'rule_of_thirds'

    def select_camera_movement(self, context: SceneContext, scene_type: str) -> str:
        """æ–‡è„ˆã«åŸºã¥ã„ãŸã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªã‚«ãƒ¡ãƒ©ãƒ ãƒ¼ãƒ–ãƒ¡ãƒ³ãƒˆé¸æŠ"""
        mood = context.mood.lower()
        pacing = context.pacing
        intensity = context.intensity
        
        candidates = []
        
        # ãƒ ãƒ¼ãƒ‰å„ªå…ˆé¸æŠ
        if mood in self.MOOD_MOVEMENT_MATRIX:
            mood_prefs = self.MOOD_MOVEMENT_MATRIX[mood]
            candidates.extend(mood_prefs['primary'])
        
        # ãƒšãƒ¼ã‚·ãƒ³ã‚°èª¿æ•´
        if pacing == 'fast':
            fast_movements = ['tracking', 'handheld', 'quick_pan', 'fast_zoom']
            candidates = [c for c in candidates if c in fast_movements] or fast_movements[:2]
        elif pacing == 'slow':
            slow_movements = ['static', 'slow_dolly', 'slow_zoom_in', 'gentle_pan']
            candidates = [c for c in candidates if c in slow_movements] or slow_movements[:2]
        
        # å¼·åº¦èª¿æ•´
        if intensity > 0.8:
            intense_movements = ['handheld', 'fast_zoom', 'dynamic_tracking']
            candidates.extend(intense_movements)
        
        return random.choice(candidates) if candidates else 'static'


class JPOPEmotionalStructure:
    """J-popæ§‹æˆã«ã‚ˆã‚‹æ„Ÿæƒ…è¨­è¨ˆã‚·ã‚¹ãƒ†ãƒ """
    
    @staticmethod
    def analyze_story_for_jpop_structure(story: str, duration: int, num_cuts: int) -> List[Dict]:
        """ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’J-popæ§‹æˆã«åˆ†æ"""
        # åŸºæœ¬4æ§‹æˆ: ã‚µãƒ“â†’Aãƒ¡ãƒ­â†’Bãƒ¡ãƒ­â†’ã‚µãƒ“
        base_structure = [
            EmotionalPhase.SABI_HOOK,     # å†’é ­ãƒ•ãƒƒã‚¯ï¼ˆ7-10ç§’ï¼‰
            EmotionalPhase.A_MELO,        # å±•é–‹ãƒ»èª¬æ˜ï¼ˆ7-10ç§’ï¼‰
            EmotionalPhase.B_MELO,        # æ·±åŒ–ãƒ»å¤šæ§˜æ€§ï¼ˆ7-10ç§’ï¼‰
            EmotionalPhase.SABI_CLIMAX    # ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹ï¼ˆ6-8ç§’ï¼‰
        ]
        
        # J-popç†æƒ³æ™‚é–“é…åˆ†ï¼ˆPDFã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æº–æ‹ ï¼‰
        ideal_durations = JPOPEmotionalStructure._calculate_ideal_phase_durations(duration)
        
        # ã‚«ãƒƒãƒˆæ•°åˆ†é…ï¼ˆæ™‚é–“æ¯”ç‡ã«åŸºã¥ãï¼‰
        phase_cuts = []
        total_assigned_cuts = 0
        
        for i, (phase, phase_duration) in enumerate(zip(base_structure, ideal_durations)):
            # æ™‚é–“æ¯”ç‡ã§ã‚«ãƒƒãƒˆæ•°ã‚’è¨ˆç®—
            time_ratio = phase_duration / duration
            phase_cut_count = max(1, round(num_cuts * time_ratio))
            
            # æœ€å¾Œã®ãƒ•ã‚§ãƒ¼ã‚ºã§èª¿æ•´
            if i == len(base_structure) - 1:
                phase_cut_count = num_cuts - total_assigned_cuts
            
            cut_duration = max(3, phase_duration // phase_cut_count)
            
            for j in range(phase_cut_count):
                cut_data = JPOPEmotionalStructure._create_phase_cut(
                    phase, j, cut_duration, story
                )
                phase_cuts.append(cut_data)
            
            total_assigned_cuts += phase_cut_count
        
        return phase_cuts

    @staticmethod
    def _calculate_ideal_phase_durations(total_duration: int) -> List[int]:
        """J-popç†æƒ³æ§‹æˆã«åŸºã¥ãæ™‚é–“é…åˆ†è¨ˆç®—"""
        if total_duration <= 30:
            # çŸ­æ™‚é–“å‹•ç”»ï¼šå‡ç­‰4åˆ†å‰²
            base_duration = total_duration // 4
            return [base_duration, base_duration, base_duration, total_duration - (base_duration * 3)]
        
        elif total_duration <= 45:
            # ä¸­æ™‚é–“å‹•ç”»ï¼šç†æƒ³æ¯”ç‡ã‚’ç¶­æŒ
            # ã‚µãƒ“1: 9ç§’, Aãƒ¡ãƒ­: 11ç§’, Bãƒ¡ãƒ­: 13ç§’, ã‚µãƒ“2: 12ç§’
            ratio = total_duration / 45
            return [
                max(6, round(9 * ratio)),   # ã‚µãƒ“1ï¼ˆãƒ•ãƒƒã‚¯ï¼‰
                max(8, round(11 * ratio)),  # Aãƒ¡ãƒ­
                max(8, round(13 * ratio)),  # Bãƒ¡ãƒ­  
                max(6, round(12 * ratio))   # ã‚µãƒ“2ï¼ˆã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹ï¼‰
            ]
        
        elif total_duration <= 90:
            # é•·æ™‚é–“å‹•ç”»ï¼šæ‹¡å¼µæ§‹æˆ
            # ã‚µãƒ“1: 10ç§’, Aãƒ¡ãƒ­: 20ç§’, Bãƒ¡ãƒ­: 35ç§’, ã‚µãƒ“2: 25ç§’
            remaining = total_duration
            sabi1 = min(12, max(8, total_duration // 8))      # ãƒ•ãƒƒã‚¯: 8-12ç§’
            sabi2 = min(15, max(10, total_duration // 6))     # ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹: 10-15ç§’
            remaining -= (sabi1 + sabi2)
            
            # æ®‹ã‚Šã‚’Aãƒ¡ãƒ­:Bãƒ¡ãƒ­ = 2:3ã§é…åˆ†
            amelo = remaining * 2 // 5
            bmelo = remaining - amelo
            
            return [sabi1, amelo, bmelo, sabi2]
        
        else:
            # è¶…é•·æ™‚é–“å‹•ç”»ï¼ˆ90ç§’è¶…ï¼‰ï¼šæ‹¡å¼µæ§‹æˆ + è¤‡æ•°ã‚µã‚¤ã‚¯ãƒ«æ¤œè¨
            # åŸºæœ¬çš„ã«ã¯90ç§’ã‚±ãƒ¼ã‚¹ã‚’æ‹¡å¼µ
            base_90 = JPOPEmotionalStructure._calculate_ideal_phase_durations(90)
            scale_factor = total_duration / 90
            return [max(8, round(d * scale_factor)) for d in base_90]

    @staticmethod
    def _create_phase_cut(phase: EmotionalPhase, cut_index: int, duration: int, story: str) -> Dict:
        """ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã‚«ãƒƒãƒˆä½œæˆ"""
        phase_characteristics = {
            EmotionalPhase.SABI_HOOK: {
                'intensity': 0.8,
                'pacing': 'fast',
                'mood': 'energetic',
                'scene_types': ['opening', 'hook', 'attention_grab'],
                'emotion_target': 'immediate_impact'
            },
            EmotionalPhase.A_MELO: {
                'intensity': 0.4,
                'pacing': 'medium',
                'mood': 'explanatory',
                'scene_types': ['character', 'context', 'development'],
                'emotion_target': 'understanding'
            },
            EmotionalPhase.B_MELO: {
                'intensity': 0.6,
                'pacing': 'medium',
                'mood': 'deepening',
                'scene_types': ['complexity', 'variation', 'buildup'],
                'emotion_target': 'engagement'
            },
            EmotionalPhase.SABI_CLIMAX: {
                'intensity': 1.0,
                'pacing': 'fast',
                'mood': 'climactic',
                'scene_types': ['climax', 'resolution', 'impact'],
                'emotion_target': 'maximum_impact'
            }
        }
        
        char = phase_characteristics[phase]
        scene_type = char['scene_types'][cut_index % len(char['scene_types'])]
        
        return {
            'duration': duration,
            'scene_description': f"{phase.value.replace('_', ' ').title()} scene {cut_index + 1}",
            'action': f"Action for {phase.value} phase",
            'scene_type': scene_type,
            'mood': char['mood'],
            'emotional_phase': phase,
            'intensity': char['intensity'],
            'pacing': char['pacing'],
            'emotion_target': char['emotion_target']
        }


class VerticalOptimizer:
    """ç¸¦å‹9:16æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    @staticmethod
    def optimize_for_vertical(base_prompt: str, composition: str, context: SceneContext) -> str:
        """ç¸¦å‹å‘ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–"""
        vertical_optimizations = {
            'aspect_ratio': '9:16 vertical aspect ratio',
            'flow_direction': 'vertical visual flow from top to bottom',
            'framing': 'tight framing suitable for mobile viewing',
            'focal_point': 'subject in upper two-thirds for natural eye flow',
            'text_space': 'space at bottom for text/UI elements',
            'background': 'simplified background to focus attention'
        }
        
        # åŸºæœ¬çš„ãªç¸¦å‹æŒ‡å®š
        optimized_prompt = base_prompt.replace('16:9', '9:16')
        
        # ç¸¦å‹ç‰¹æœ‰ã®è¦ç´ ã‚’è¿½åŠ 
        additions = [
            vertical_optimizations['aspect_ratio'],
            vertical_optimizations['flow_direction']
        ]
        
        # æ§‹å›³ã«å¿œã˜ãŸèª¿æ•´
        if composition in ['rule_of_thirds', 'golden_ratio']:
            additions.append(vertical_optimizations['focal_point'])
        
        if composition == 'centered':
            additions.append('vertically centered composition for mobile impact')
        
        # ã‚¯ãƒ­ãƒ¼ã‚ºã‚¢ãƒƒãƒ—ã®å¼·åŒ–ï¼ˆç¸¦å‹ã§åŠ¹æœçš„ï¼‰
        if any(keyword in base_prompt.lower() for keyword in ['close-up', 'face', 'portrait']):
            additions.append('enhanced close-up framing for vertical viewing')
        
        return base_prompt + ', ' + ', '.join(additions)


class EmotionalEngagementEnhancer:
    """æ„Ÿæƒ…ç§»å…¥ä¿ƒé€²æ©Ÿèƒ½"""
    
    # å…±æ„Ÿèµ·ç‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
    EMPATHY_HOOKS = {
        'daily_life': ['morning routine everyone knows', 'familiar workplace scenario'],
        'universal_emotion': ['feeling lost in a new place', 'excitement before adventure'],
        'relatable_challenge': ['trying to decide what to do', 'looking for something special'],
        'sensory_connection': ['warm sunlight on skin', 'cool ocean breeze']
    }
    
    # 1-2ç§’1ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ†ãƒ³ãƒè¨­è¨ˆ
    ACTION_PACING = {
        'fast': {'actions_per_second': 1.0, 'cut_style': 'quick_cuts'},
        'medium': {'actions_per_second': 0.7, 'cut_style': 'natural_flow'},
        'slow': {'actions_per_second': 0.5, 'cut_style': 'contemplative'}
    }
    
    @staticmethod
    def enhance_emotional_connection(cut_data: Dict, context: SceneContext) -> Dict:
        """æ„Ÿæƒ…çš„æ¥ç¶šã®å¼·åŒ–"""
        phase = cut_data.get('emotional_phase', EmotionalPhase.A_MELO)
        
        if phase == EmotionalPhase.SABI_HOOK:
            # å†’é ­ã§å…±æ„Ÿèµ·ç‚¹ã‚’è¨­å®š
            empathy_type = random.choice(list(EmotionalEngagementEnhancer.EMPATHY_HOOKS.keys()))
            hook = random.choice(EmotionalEngagementEnhancer.EMPATHY_HOOKS[empathy_type])
            cut_data['empathy_hook'] = hook
            cut_data['scene_description'] = f"Opening with relatable moment: {hook}"
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ãƒã®èª¿æ•´
        pacing = cut_data.get('pacing', 'medium')
        tempo_info = EmotionalEngagementEnhancer.ACTION_PACING.get(pacing, 
                     EmotionalEngagementEnhancer.ACTION_PACING['medium'])
        
        cut_data['action_tempo'] = tempo_info
        cut_data['recommended_cuts_per_beat'] = tempo_info['actions_per_second']
        
        return cut_data

    @staticmethod
    def generate_three_layer_stimulation(cut_data: Dict) -> Dict:
        """3å±¤åˆºæ¿€ï¼ˆè¦–è¦šãƒ»è´è¦šãƒ»æ„Ÿæƒ…ï¼‰ã®ç”Ÿæˆ"""
        stimulation_layers = {
            'visual': EmotionalEngagementEnhancer._generate_visual_stimulation(cut_data),
            'auditory': EmotionalEngagementEnhancer._generate_auditory_cues(cut_data),
            'emotional': EmotionalEngagementEnhancer._generate_emotional_triggers(cut_data)
        }
        
        cut_data['three_layer_stimulation'] = stimulation_layers
        return cut_data
    
    @staticmethod
    def _generate_visual_stimulation(cut_data: Dict) -> Dict:
        """è¦–è¦šåˆºæ¿€ã®ç”Ÿæˆ"""
        intensity = cut_data.get('intensity', 0.5)
        
        if intensity > 0.7:
            return {
                'movement': 'dynamic motion and color changes',
                'color': 'high contrast and vibrant colors',
                'effects': 'motion blur and dynamic lighting'
            }
        elif intensity > 0.4:
            return {
                'movement': 'gentle movement and smooth transitions',
                'color': 'warm and inviting color palette',
                'effects': 'soft lighting and natural flow'
            }
        else:
            return {
                'movement': 'minimal movement for focus',
                'color': 'calm and soothing colors',
                'effects': 'stable lighting and clear focus'
            }
    
    @staticmethod
    def _generate_auditory_cues(cut_data: Dict) -> Dict:
        """è´è¦šæ‰‹ãŒã‹ã‚Šã®ç”Ÿæˆ"""
        mood = cut_data.get('mood', 'neutral')
        phase = cut_data.get('emotional_phase', EmotionalPhase.A_MELO)
        
        audio_suggestions = {
            'bgm_style': EmotionalEngagementEnhancer._suggest_bgm_style(mood, phase),
            'sfx_needs': EmotionalEngagementEnhancer._suggest_sound_effects(cut_data),
            'rhythm_sync': 'sync visual cuts with musical beats'
        }
        
        return audio_suggestions
    
    @staticmethod
    def _generate_emotional_triggers(cut_data: Dict) -> Dict:
        """æ„Ÿæƒ…çš„ãƒˆãƒªã‚¬ãƒ¼ã®ç”Ÿæˆ"""
        return {
            'surprise_element': 'unexpected reveal or transition',
            'connection_point': 'moment for viewer identification',
            'satisfaction_hook': 'visual or narrative payoff'
        }
    
    @staticmethod
    def _suggest_bgm_style(mood: str, phase: EmotionalPhase) -> str:
        """BGMã‚¹ã‚¿ã‚¤ãƒ«ã®ææ¡ˆ"""
        style_matrix = {
            'energetic': 'upbeat and driving rhythm',
            'peaceful': 'gentle and flowing melody',
            'mysterious': 'ambient and atmospheric',
            'dramatic': 'building orchestral tension',
            'joyful': 'bright and uplifting'
        }
        
        base_style = style_matrix.get(mood, 'balanced instrumental')
        
        # ãƒ•ã‚§ãƒ¼ã‚ºã«ã‚ˆã‚‹èª¿æ•´
        if phase == EmotionalPhase.SABI_HOOK:
            return f"{base_style} with strong opening hook"
        elif phase == EmotionalPhase.SABI_CLIMAX:
            return f"{base_style} building to emotional peak"
        
        return base_style
    
    @staticmethod
    def _suggest_sound_effects(cut_data: Dict) -> List[str]:
        """åŠ¹æœéŸ³ã®ææ¡ˆ"""
        scene_type = cut_data.get('scene_type', 'dialogue')
        
        sfx_library = {
            'opening': ['ambient atmosphere', 'subtle music swell'],
            'action': ['movement sounds', 'impact effects'],
            'character': ['footsteps', 'clothing rustle'],
            'climax': ['dramatic sting', 'emotional crescendo']
        }
        
        return sfx_library.get(scene_type, ['natural ambient sound'])


class EnhancedStoryboardGenerator(CoreStoryboardGenerator):
    """é«˜åº¦åŒ–ã•ã‚ŒãŸã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        é«˜åº¦åŒ–ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–
        
        Args:
            config: è¨­å®šè¾æ›¸ï¼ˆå¾“æ¥ã®GeneratorConfigã«åŠ ãˆã¦æ–°æ©Ÿèƒ½è¨­å®šï¼‰
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®æ‹¡å¼µ
        default_enhanced_config = {
            'aspect_ratio': AspectRatio.VERTICAL,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç¸¦å‹
            'genre': VideoGenre.TOURISM,
            'emotional_structure': 'jpop',
            'intelligence_level': 'high',  # low, medium, high
            'empathy_enhancement': True,
            'three_layer_stimulation': True,
            'vertical_optimization': True
        }
        
        if config:
            default_enhanced_config.update(config)
        
        super().__init__()
        self.enhanced_config = default_enhanced_config
        self.selection_engine = IntelligentSelectionEngine()
        
    def generate_storyboard(self, input_data: Dict) -> StoryboardData:
        """é«˜åº¦åŒ–ã•ã‚ŒãŸã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ç”Ÿæˆ"""
        story_description = input_data.get('story_description', '')
        visual_analysis = input_data.get('visual_analysis')
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ‹¡å¼µè§£æ
        context = self._analyze_enhanced_context(input_data)
        
        print("\nğŸ§  Enhanced analysis...")
        print(f"  ğŸ“Š Genre: {context.genre.value}")
        print(f"  ğŸ“± Aspect Ratio: {context.aspect_ratio.value}")
        print(f"  ğŸ’¡ Intelligence Level: {self.enhanced_config['intelligence_level']}")
        
        # ãƒ•ãƒƒã‚¯ã®äº‹å‰ç”Ÿæˆ
        input_data = self.trigger_hook('pre_generation', input_data)
        
        # J-popæ§‹æˆã«ã‚ˆã‚‹æ„Ÿæƒ…è¨­è¨ˆ
        print("\nğŸµ Applying J-pop emotional structure...")
        cuts_data = JPOPEmotionalStructure.analyze_story_for_jpop_structure(
            story_description, 
            self.config.duration, 
            self.config.num_cuts
        )
        
        # ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé¸æŠã«ã‚ˆã‚‹é«˜åº¦åŒ–
        print(f"\nğŸ¯ Creating {len(cuts_data)} cuts with intelligent selection...")
        cuts = []
        for i, cut_info in enumerate(cuts_data):
            # ã‚·ãƒ¼ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
            scene_context = SceneContext(
                emotional_phase=cut_info.get('emotional_phase', EmotionalPhase.A_MELO),
                intensity=cut_info.get('intensity', 0.5),
                pacing=cut_info.get('pacing', 'medium'),
                mood=cut_info.get('mood', 'neutral'),
                genre=context.genre,
                aspect_ratio=context.aspect_ratio
            )
            
            # ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé¸æŠ
            cut = self._create_enhanced_cut(
                i + 1,
                cut_info,
                scene_context,
                visual_analysis
            )
            
            # æ„Ÿæƒ…ç§»å…¥ä¿ƒé€²æ©Ÿèƒ½
            if self.enhanced_config.get('empathy_enhancement', True):
                cut_dict = cut.to_dict()
                cut_dict = EmotionalEngagementEnhancer.enhance_emotional_connection(
                    cut_dict, scene_context
                )
                
                if self.enhanced_config.get('three_layer_stimulation', True):
                    cut_dict = EmotionalEngagementEnhancer.generate_three_layer_stimulation(cut_dict)
                
                # CutDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ›´æ–°
                for key, value in cut_dict.items():
                    if hasattr(cut, key):
                        setattr(cut, key, value)
            
            cuts.append(cut)
            phase_name = cut_info.get('emotional_phase', EmotionalPhase.A_MELO).value.replace('_', ' ').title()
            print(f"  âœ“ Cut {i + 1} ({phase_name}): {cut.scene_description[:50]}...")
        
        # é«˜åº¦åŒ–ã•ã‚ŒãŸã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ä½œæˆ
        storyboard = StoryboardData(
            title=self.config.title,
            duration=self.config.duration,
            num_cuts=len(cuts),
            cuts=cuts,
            style_guide=self._create_enhanced_style_guide(visual_analysis, context),
            key_visual_analysis=visual_analysis,
            created_at=datetime.now().isoformat()
        )
        
        # ãƒã‚¹ãƒˆç”Ÿæˆãƒ•ãƒƒã‚¯
        storyboard_dict = storyboard.to_dict()
        storyboard_dict = self.trigger_hook('post_generation', storyboard_dict)
        
        print(f"\nâœ… Enhanced storyboard generation complete!")
        print(f"   ğŸ“± Optimized for {context.aspect_ratio.value}")
        print(f"   ğŸ­ {context.genre.value.title()} genre")
        print(f"   ğŸµ J-pop emotional structure applied")
        
        return storyboard

    def _analyze_enhanced_context(self, input_data: Dict) -> SceneContext:
        """æ‹¡å¼µã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè§£æ"""
        # åŸºæœ¬å€¤ã®æŠ½å‡º
        genre = VideoGenre(self.enhanced_config.get('genre', 'tourism'))
        aspect_ratio = AspectRatio(self.enhanced_config.get('aspect_ratio', '9:16'))
        
        # ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‹ã‚‰ã®è‡ªå‹•æ¨è«–
        story = input_data.get('story_description', '').lower()
        
        # ã‚¸ãƒ£ãƒ³ãƒ«è‡ªå‹•æ¨è«–
        if 'education' in story or 'learn' in story or 'teach' in story:
            genre = VideoGenre.EDUCATIONAL
        elif 'product' in story or 'buy' in story or 'brand' in story:
            genre = VideoGenre.COMMERCIAL
        elif 'music' in story or 'song' in story or 'concert' in story:
            genre = VideoGenre.MUSIC
        elif 'travel' in story or 'visit' in story or 'destination' in story:
            genre = VideoGenre.TOURISM
        
        return SceneContext(
            emotional_phase=EmotionalPhase.A_MELO,  # åˆæœŸå€¤
            intensity=0.5,
            pacing='medium',
            mood='neutral',
            genre=genre,
            aspect_ratio=aspect_ratio,
            target_audience=input_data.get('target_audience', 'general')
        )

    def _create_enhanced_cut(
        self, 
        cut_number: int, 
        cut_info: Dict, 
        context: SceneContext, 
        visual_analysis: Optional[Dict]
    ) -> CutData:
        """é«˜åº¦åŒ–ã•ã‚ŒãŸã‚«ãƒƒãƒˆä½œæˆ"""
        scene_type = cut_info.get('scene_type', 'dialogue')
        
        # ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé¸æŠ
        camera_angle = self.selection_engine.select_camera_angle(context, scene_type)
        composition = self.selection_engine.select_composition(context, scene_type)
        camera_movement = self.selection_engine.select_camera_movement(context, scene_type)
        
        # é«˜åº¦åŒ–ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        image_prompt = self._generate_enhanced_image_prompt(
            cut_info, camera_angle, composition, visual_analysis, context
        )
        
        # ç¸¦å‹æœ€é©åŒ–
        if context.aspect_ratio == AspectRatio.VERTICAL and self.enhanced_config.get('vertical_optimization', True):
            image_prompt = VerticalOptimizer.optimize_for_vertical(
                image_prompt, composition, context
            )
        
        # é«˜åº¦åŒ–ãƒ“ãƒ‡ã‚ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        veo3_prompt = self._generate_enhanced_veo3_prompt(cut_info, camera_movement, context)
        sora2_prompt = self._generate_enhanced_sora2_prompt(cut_info, camera_movement, context)
        
        # ãƒ¢ãƒ‡ãƒ«æ¨å¥¨ã®é«˜åº¦åŒ–
        recommended_model = self._select_optimal_model(context, cut_info)
        
        return CutData(
            cut_number=cut_number,
            duration=cut_info.get('duration', 8),
            scene_description=cut_info.get('scene_description', ''),
            action=cut_info.get('action', ''),
            composition=composition,
            camera_angle=camera_angle,
            camera_movement=camera_movement,
            lighting=self._determine_enhanced_lighting(cut_info.get('mood', 'neutral'), context),
            mood=cut_info.get('mood', 'neutral'),
            image_prompt=image_prompt,
            veo3_prompt=veo3_prompt,
            sora2_prompt=sora2_prompt,
            recommended_model=recommended_model
        )

    def _generate_enhanced_image_prompt(
        self, 
        cut_info: Dict, 
        camera_angle: str, 
        composition: str, 
        visual_analysis: Optional[Dict],
        context: SceneContext
    ) -> str:
        """é«˜åº¦åŒ–ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        # åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        base_prompt = super()._generate_image_prompt(cut_info, camera_angle, composition, visual_analysis)
        
        # ç¸¦å‹æŒ‡å®šã«å¤‰æ›´
        if context.aspect_ratio == AspectRatio.VERTICAL:
            base_prompt = base_prompt.replace('16:9', '9:16')
        
        # ã‚¸ãƒ£ãƒ³ãƒ«ç‰¹åŒ–ã®è¿½åŠ 
        genre_enhancements = {
            VideoGenre.TOURISM: 'travel photography style, destination appeal, inviting atmosphere',
            VideoGenre.EDUCATIONAL: 'clear and instructional, professional presentation',
            VideoGenre.COMMERCIAL: 'high production value, premium quality, market appeal',
            VideoGenre.DOCUMENTARY: 'authentic and realistic, natural lighting',
            VideoGenre.MUSIC: 'dynamic and rhythmic, artistic composition',
            VideoGenre.NARRATIVE: 'cinematic storytelling, emotional depth'
        }
        
        enhancement = genre_enhancements.get(context.genre, '')
        if enhancement:
            base_prompt += f", {enhancement}"
        
        # å¼·åº¦ãƒ™ãƒ¼ã‚¹èª¿æ•´
        intensity_enhancements = {
            0.8: "high impact, dramatic emphasis, strong visual presence",
            0.6: "moderate intensity, engaging composition, clear focus",
            0.3: "subtle and gentle, soft approach, understated elegance"
        }
        
        # æœ€ã‚‚è¿‘ã„å¼·åº¦ã‚’é¸æŠ
        intensity_key = min(intensity_enhancements.keys(), 
                           key=lambda x: abs(x - context.intensity))
        base_prompt += f", {intensity_enhancements[intensity_key]}"
        
        return base_prompt

    def _generate_enhanced_veo3_prompt(
        self, 
        cut_info: Dict, 
        camera_movement: str, 
        context: SceneContext
    ) -> str:
        """é«˜åº¦åŒ–Veo3ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        duration = cut_info.get('duration', 8)
        
        # ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ã®ãƒ ãƒ¼ãƒ–ãƒ¡ãƒ³ãƒˆè¨˜è¿°
        movement_descriptions = {
            'static': f"Camera: Static shot with natural breathing movement, {duration} seconds",
            'slow_zoom_in': f"Camera: Gradual zoom in over {duration} seconds, building intimacy",
            'slow_pull_back': f"Camera: Slow pull back revealing context, {duration} seconds",
            'tracking': f"Camera: Smooth tracking shot following subject, {duration} seconds",
            'handheld': f"Camera: Natural handheld movement with {context.pacing} energy, {duration} seconds",
            'gentle_pan': f"Camera: Smooth pan across scene over {duration} seconds",
            'crane': f"Camera: Majestic crane movement revealing scale, {duration} seconds"
        }
        
        movement_desc = movement_descriptions.get(camera_movement, 
                       f"Camera: {camera_movement} movement, {duration} seconds")
        
        # æ„Ÿæƒ…ãƒ•ã‚§ãƒ¼ã‚ºå¯¾å¿œ
        phase_enhancements = {
            EmotionalPhase.SABI_HOOK: "with strong opening impact, immediate viewer engagement",
            EmotionalPhase.A_MELO: "with steady development, clear narrative progression", 
            EmotionalPhase.B_MELO: "with building complexity, layered visual interest",
            EmotionalPhase.SABI_CLIMAX: "with maximum impact, emotional crescendo"
        }
        
        phase = cut_info.get('emotional_phase', EmotionalPhase.A_MELO)
        phase_enhancement = phase_enhancements.get(phase, '')
        
        # 3å±¤åˆºæ¿€å¯¾å¿œ
        stimulation = cut_info.get('three_layer_stimulation', {})
        if stimulation:
            visual_stim = stimulation.get('visual', {})
            movement_enhancement = visual_stim.get('movement', 'natural movement')
        else:
            movement_enhancement = 'natural cinematic movement'
        
        prompt_parts = [
            movement_desc,
            f"Action: {cut_info.get('action', 'scene development')}",
            f"Visual enhancement: {movement_enhancement}",
            f"Mood: {context.mood} atmosphere",
            phase_enhancement,
            "Maintain composition and reference image consistency"
        ]
        
        return '. '.join(filter(None, prompt_parts)) + '.'

    def _generate_enhanced_sora2_prompt(
        self, 
        cut_info: Dict, 
        camera_movement: str, 
        context: SceneContext
    ) -> str:
        """é«˜åº¦åŒ–Sora2ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        duration = cut_info.get('duration', 8)
        
        # ã‚ˆã‚Šè‡ªç„¶è¨€èªçš„ãªè¨˜è¿°ï¼ˆSora2å‘ã‘ï¼‰
        movement_descriptions = {
            'static': f"The camera remains perfectly still for {duration} seconds, allowing the scene to breathe naturally",
            'slow_zoom_in': f"The camera slowly zooms in over {duration} seconds, gradually drawing the viewer closer to the subject",
            'tracking': f"The camera smoothly follows the movement through {duration} seconds, maintaining perfect framing",
            'handheld': f"The camera captures {duration} seconds with organic handheld movement, adding authentic human perspective",
            'crane': f"The camera sweeps majestically for {duration} seconds, revealing the grand scope of the scene"
        }
        
        movement_desc = movement_descriptions.get(camera_movement,
                       f"The camera moves with {camera_movement} for {duration} seconds")
        
        # ã‚¹ãƒˆãƒ¼ãƒªãƒ¼è¦ç´ ã®çµ±åˆ
        scene_desc = cut_info.get('scene_description', '')
        action = cut_info.get('action', 'natural scene progression')
        
        # æ„Ÿæƒ…çš„æ–‡è„ˆã®è¿½åŠ 
        emotional_context = {
            EmotionalPhase.SABI_HOOK: f"This opening {duration}-second sequence immediately captures attention",
            EmotionalPhase.A_MELO: f"This {duration}-second development builds understanding", 
            EmotionalPhase.B_MELO: f"This {duration}-second sequence adds depth and complexity",
            EmotionalPhase.SABI_CLIMAX: f"This {duration}-second climax delivers maximum emotional impact"
        }
        
        phase = cut_info.get('emotional_phase', EmotionalPhase.A_MELO)
        emotional_intro = emotional_context.get(phase, f"This {duration}-second sequence")
        
        # ç¸¦å‹é…æ…®
        aspect_consideration = ""
        if context.aspect_ratio == AspectRatio.VERTICAL:
            aspect_consideration = "The vertical composition guides the viewer's eye naturally from top to bottom. "
        
        prompt = f"""{emotional_intro} featuring {scene_desc}. {action}. {movement_desc}. {aspect_consideration}The scene maintains {context.mood} atmosphere with {context.pacing} pacing, ensuring visual consistency with the reference image throughout the sequence.""".strip()
        
        return prompt

    def _select_optimal_model(self, context: SceneContext, cut_info: Dict) -> str:
        """æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠ"""
        # æ–‡è„ˆã«åŸºã¥ããƒ¢ãƒ‡ãƒ«æ¨å¥¨
        if context.genre == VideoGenre.COMMERCIAL:
            return 'Veo 3'  # é«˜å“è³ªé‡è¦–
        elif context.intensity > 0.7:
            return 'Sora 2'  # ãƒ‰ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯è¡¨ç¾
        elif context.pacing == 'slow':
            return 'Veo 3'  # å¾®ç´°ãªå‹•ã
        else:
            return 'Veo 3'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    def _determine_enhanced_lighting(self, mood: str, context: SceneContext) -> str:
        """é«˜åº¦åŒ–ç…§æ˜æ±ºå®š"""
        # åŸºæœ¬çš„ãªç…§æ˜ãƒãƒƒãƒ”ãƒ³ã‚°
        base_lighting = super()._determine_lighting(mood)
        
        # ã‚¸ãƒ£ãƒ³ãƒ«è£œæ­£
        genre_lighting = {
            VideoGenre.COMMERCIAL: 'premium studio lighting',
            VideoGenre.DOCUMENTARY: 'natural authentic lighting', 
            VideoGenre.TOURISM: 'golden hour travel lighting',
            VideoGenre.EDUCATIONAL: 'clear instructional lighting'
        }
        
        enhancement = genre_lighting.get(context.genre, '')
        if enhancement:
            return f"{base_lighting}, {enhancement}"
        
        return base_lighting

    def _create_enhanced_style_guide(
        self, 
        visual_analysis: Optional[Dict], 
        context: SceneContext
    ) -> Dict:
        """é«˜åº¦åŒ–ã‚¹ã‚¿ã‚¤ãƒ«ã‚¬ã‚¤ãƒ‰ä½œæˆ"""
        base_guide = super()._create_style_guide(visual_analysis)
        
        # é«˜åº¦åŒ–è¦ç´ ã®è¿½åŠ 
        enhanced_elements = {
            'aspect_ratio': context.aspect_ratio.value,
            'genre_optimization': context.genre.value,
            'emotional_structure': 'j-pop_four_phase',
            'intelligence_features': {
                'mood_based_selection': True,
                'context_aware_composition': True,
                'vertical_optimization': self.enhanced_config.get('vertical_optimization', True),
                'three_layer_stimulation': self.enhanced_config.get('three_layer_stimulation', True)
            },
            'targeting': {
                'audience': context.target_audience,
                'platform': 'mobile_first' if context.aspect_ratio == AspectRatio.VERTICAL else 'multi_platform',
                'engagement_strategy': 'j_pop_emotional_flow'
            }
        }
        
        base_guide.update(enhanced_elements)
        return base_guide